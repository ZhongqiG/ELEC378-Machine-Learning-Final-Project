{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtOOW7Pnxin7",
        "outputId": "c57d1769-f9fb-4a05-9b2b-243701a93bb1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from scipy.io import wavfile\n",
        "import re\n",
        "import seaborn as sns\n",
        "import random\n",
        "from sklearn import svm\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils.np_utils import to_categorical\n"
      ],
      "metadata": {
        "id": "a-OF3nfHOSmz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test data and test data directory \n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/ELEC378_FinalProject/data/data\"\n",
        "\n",
        "test_dir = \"/content/drive/MyDrive/ELEC378_FinalProject/test/test\"\n",
        "\n"
      ],
      "metadata": {
        "id": "HSN_P0dCypm1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Containis function for feature extraction. Takes a directory and a boolean indicating wheather to split the data or not. For cross validation use the data directory (labeled data) and split = true. Run all the way to the cell that prints out accuracy score. For testing model on the unlabled data in test folder (those that are named sample001, etc), run the cells below the accuracy score cell will give you a .csv file ready for submission on kaggle. "
      ],
      "metadata": {
        "id": "wMRWN6ilHChk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Feature Extraction func\n",
        "def FeatureExtraction (dir, split=True): \n",
        " \n",
        "  '''\n",
        "  Function for feature extraction. Change this section to change what features we are using. \n",
        "\n",
        "  input: a directory of the data. Will split to test and train, a boolean: if split is true, will extract features and labels and store them in 4 arrays:\n",
        "  train_data, train_label, test_data, test_label. if split is false, function will extract all data provided in the directory and output 2 arrays with values 2 arrays that are empty. \n",
        "  Use labeled data and split = false will give you  (train_data, train_label); use unlabled data and split = false will give you (test_data, test_label).\n",
        "\n",
        "  output: data matrix and lables for both test and train. \n",
        "\n",
        "  '''\n",
        "  audio_files = [f for f in os.listdir(dir) if f.endswith(\".wav\")]\n",
        "  \n",
        "  train_files, test_files = train_test_split(audio_files, test_size=0.2)\n",
        "\n",
        "  train_data = []\n",
        "  train_label= []\n",
        "  test_data = []\n",
        "  test_label = []  \n",
        "  \n",
        "  \n",
        "  for file_name in audio_files:\n",
        "\n",
        "      # Load audio\n",
        "      file_path = os.path.join(dir, file_name)\n",
        "      raw_audio, sr = librosa.load(file_path)\n",
        "\n",
        "      # if audio is too short, append zeros after it. \n",
        "      if np.shape(raw_audio)[0] < 80000: \n",
        "          padded_audio = np.pad(raw_audio, [(0, 80000 - np.shape(raw_audio)[0])], mode='constant')\n",
        "      else:\n",
        "          padded_audio = raw_audio\n",
        "\n",
        "      # now slice so that we guarantee that each array has the same length\n",
        "      audio = padded_audio[20000:80000]\n",
        "\n",
        "      # feature extraction\n",
        "\n",
        "      mfccs = librosa.feature.mfcc(y=audio, sr = 22050, n_mfcc = 100)\n",
        "\n",
        "      #chroma_stft = librosa.feature.chroma_stft(y = audio, sr=22050)\n",
        "      #mel_spec = librosa.feature.melspectrogram(y = audio, sr = 22050, n_mels = 5)\n",
        "      #gfccs = librosa.feature.gfcc(y, sr=sr, n_mfcc=20)\n",
        "\n",
        "      # Concatenate\n",
        "      #features = np.concatenate([mfccs.flatten()])\n",
        "      features = mfccs\n",
        "\n",
        "      label_map = {'angry': 0, 'calm': 1, 'disgust': 2, 'fearful': 3, 'happy': 4, 'neutral': 5, 'sad': 6, 'surprised': 7}\n",
        "      label = file_name.split(\".\")[0]\n",
        "\n",
        "      #unlabeled data\n",
        "      if label[:-3] == \"sample\":\n",
        "        test_data.append(features)\n",
        "        test_label.append(label)\n",
        "\n",
        "      #labeled data\n",
        "      else:\n",
        "        if split: \n",
        "          if file_name in train_files:\n",
        "            train_data.append(features)\n",
        "            label = re.sub('[^a-z]', \"\", label)\n",
        "            train_label.append(label_map[label])\n",
        "\n",
        "          elif file_name in test_files: \n",
        "            test_data.append(features)\n",
        "            label = re.sub('[^a-z]', \"\", label)\n",
        "            test_label.append(label_map[label])\n",
        "\n",
        "        elif not split: \n",
        "          \n",
        "          train_data.append(features)\n",
        "          label = re.sub('[^a-z]', \"\", label)\n",
        "          train_label.append(label_map[label])\n",
        "  \n",
        "\n",
        "  train_data = np.array(train_data)\n",
        "  train_label = np.array(train_label)\n",
        "  test_data = np.array(test_data)\n",
        "  test_label = np.array(test_label) \n",
        "\n",
        "  print(\"mfccs size is:\", np.shape(mfccs))\n",
        "  return train_data, train_label, test_data, test_label\n",
        "\n"
      ],
      "metadata": {
        "id": "Msvv8n4hfk2h",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = FeatureExtraction(data_dir, split = True)\n",
        "y_train_ohe = to_categorical(y_train)\n",
        "y_test_ohe = to_categorical(y_test)\n"
      ],
      "metadata": {
        "id": "VzAiCJ7FqnpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605eec62-cd2d-4478-c7b6-1a640304f582"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mfccs size is: (100, 118)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SVM\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "clf = make_pipeline(StandardScaler(), SVC(kernel = \"linear\"))\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_predicted= clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_predicted)\n",
        "\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "Oezh9Evx646j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "615838d8-fd26-4657-d328-37ae44ea5075"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d5e7b0e5dcf6>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \"\"\"\n\u001b[1;32m    400\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m             )\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    916\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. StandardScaler expected <= 2."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is for unlabeled test data and importing a csv\n"
      ],
      "metadata": {
        "id": "M7U35fy5TJN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = make_pipeline(StandardScaler(), SVC(kernel = \"linear\"))\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_predicted= clf.predict(X_test)"
      ],
      "metadata": {
        "id": "n1M81QJpnM2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Conv1D, Embedding, Dropout, MaxPooling1D, GlobalMaxPooling1D, Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras import Model\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "g_oeWmPAuIDX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ixOqNtpMZNDF"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_classes = 8\n",
        "def build_cnn(input_shape, num_classes):\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    # Convolutional layers\n",
        "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='valid', input_shape=input_shape))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(128, (3, 3), padding='valid'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(256, (3, 3), padding='valid'))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.MaxPooling2D())\n",
        "\n",
        "    # Dense layers\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(128))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Activation('relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.3))\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(num_classes))\n",
        "    model.add(tf.keras.layers.Activation('softmax'))\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#num frames is number of wav files\n",
        "# es = EarlyStopping(monitor='accuracy',\n",
        "#                    mode='min',\n",
        "#                    restore_best_weights=True,\n",
        "#                    patience=10,\n",
        "#                    verbose=1)\n",
        "\n",
        "#num frames is number of wav files\n",
        "model = build_cnn((X_train.shape[1], X_train.shape[2], 1), num_classes)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train_ohe, batch_size=32, epochs=15, validation_data=(X_test, y_test_ohe))\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "plt.title('Training and validation error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "t1YjWExXuf7r",
        "outputId": "4f7cbd3a-60e9-4f97-a61a-5094c3c2c135"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 1.5990 - accuracy: 0.4301"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-99713bbe558c>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_ohe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_ohe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1850\u001b[0m             )\n\u001b[1;32m   1851\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 315\n  y sizes: 225\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract all labeled data\n",
        "X_train, y_train, dummy1, dummy2 = FeatureExtraction(data_dir, split = False)\n",
        "# extract all unlabeled data\n",
        "dummy1, dummy2, X_test, y_test = FeatureExtraction(test_dir, split = False)\n",
        "y_train_ohe = to_categorical(y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViCeGe6zyu2i",
        "outputId": "e042be0b-9249-4025-9754-7ba9dc0dc920"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mfccs size is: (100, 118)\n",
            "mfccs size is: (100, 118)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell is to export a .csv file to submit (filename & predicted label) \n",
        "model = build_cnn((X_train.shape[1], X_train.shape[2], 1), num_classes)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train_ohe, batch_size=32, epochs=7)\n",
        "y_predicted = model.predict(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "L7Gjd7waqXsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "526d93c0-35ee-4384-c2f1-4be17f08cb74"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "36/36 [==============================] - 39s 34ms/step - loss: 1.7025 - accuracy: 0.4071\n",
            "Epoch 2/7\n",
            "36/36 [==============================] - 1s 32ms/step - loss: 1.0466 - accuracy: 0.6391\n",
            "Epoch 3/7\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.7384 - accuracy: 0.7573\n",
            "Epoch 4/7\n",
            "36/36 [==============================] - 1s 34ms/step - loss: 0.5933 - accuracy: 0.8231\n",
            "Epoch 5/7\n",
            "36/36 [==============================] - 1s 35ms/step - loss: 0.4191 - accuracy: 0.8871\n",
            "Epoch 6/7\n",
            "36/36 [==============================] - 1s 33ms/step - loss: 0.3525 - accuracy: 0.9084\n",
            "Epoch 7/7\n",
            "36/36 [==============================] - 1s 33ms/step - loss: 0.2419 - accuracy: 0.9467\n",
            "10/10 [==============================] - 0s 13ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_predicted_ohv = np.argmax(y_predicted, axis = 1)\n"
      ],
      "metadata": {
        "id": "Zv1Acem80Yrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6942cb-b8e7-488f-807e-d5c93bfcf320"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 6, 6, 1, 1, 6, 1, 6, 6, 1, 1, 1, 1, 1, 1, 6, 6, 2, 1, 6, 7, 1,\n",
              "       5, 6, 0, 1, 1, 1, 1, 0, 6, 1, 1, 1, 0, 6, 1, 6, 1, 0, 3, 1, 1, 1,\n",
              "       6, 1, 1, 6, 6, 1, 1, 6, 0, 0, 6, 1, 7, 1, 1, 1, 7, 3, 6, 6, 1, 6,\n",
              "       6, 0, 6, 0, 1, 6, 6, 6, 1, 6, 1, 2, 6, 6, 1, 6, 6, 7, 1, 1, 6, 1,\n",
              "       1, 6, 6, 7, 6, 6, 1, 7, 1, 1, 1, 6, 6, 1, 1, 6, 1, 7, 6, 1, 1, 1,\n",
              "       6, 1, 1, 1, 1, 7, 3, 6, 7, 0, 2, 6, 7, 6, 7, 7, 1, 6, 7, 1, 7, 7,\n",
              "       6, 7, 0, 7, 6, 6, 4, 1, 6, 1, 6, 0, 6, 7, 6, 1, 6, 1, 1, 1, 1, 6,\n",
              "       3, 1, 6, 1, 7, 6, 1, 1, 1, 7, 1, 1, 1, 2, 6, 6, 7, 1, 6, 0, 6, 0,\n",
              "       1, 6, 1, 1, 6, 0, 7, 1, 6, 1, 6, 6, 2, 0, 6, 1, 0, 6, 7, 3, 6, 6,\n",
              "       1, 6, 7, 1, 1, 0, 6, 1, 7, 6, 7, 1, 1, 1, 6, 1, 1, 6, 0, 1, 1, 1,\n",
              "       3, 1, 6, 1, 6, 1, 1, 6, 6, 6, 1, 3, 0, 6, 6, 6, 1, 1, 6, 1, 7, 7,\n",
              "       4, 1, 6, 1, 1, 1, 0, 6, 6, 6, 6, 6, 7, 0, 6, 6, 1, 6, 6, 6, 6, 1,\n",
              "       7, 6, 6, 6, 7, 6, 0, 3, 1, 1, 1, 6, 7, 7, 1, 1, 1, 1, 4, 6, 6, 6,\n",
              "       0, 6, 1, 6, 6, 1, 1, 7, 1, 6, 1, 6, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1,\n",
              "       7, 0, 1, 7, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_predicted_ohv)\n",
        "label_map = {'angry': 0, 'calm': 1, 'disgust': 2, 'fearful': 3, 'happy': 4, 'neutral': 5, 'sad': 6, 'surprised': 7}\n",
        "\n",
        "def dict_search(dictionary, value):\n",
        "    for key, val in dictionary.items():\n",
        "        if val == value:\n",
        "            return key\n",
        "\n",
        "y_predicted_label = []\n",
        "for i in y_predicted_ohv: \n",
        "    val = dict_search(label_map, i)\n",
        "    y_predicted_label.append(val)\n",
        "\n",
        "\n",
        "print(y_predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf0h_tfGKCGV",
        "outputId": "bf3e97d5-3350-4413-f7cc-c74b53fdd1da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 6 6 1 1 6 1 6 6 1 1 1 1 1 1 6 6 2 1 6 7 1 5 6 0 1 1 1 1 0 6 1 1 1 0 6 1\n",
            " 6 1 0 3 1 1 1 6 1 1 6 6 1 1 6 0 0 6 1 7 1 1 1 7 3 6 6 1 6 6 0 6 0 1 6 6 6\n",
            " 1 6 1 2 6 6 1 6 6 7 1 1 6 1 1 6 6 7 6 6 1 7 1 1 1 6 6 1 1 6 1 7 6 1 1 1 6\n",
            " 1 1 1 1 7 3 6 7 0 2 6 7 6 7 7 1 6 7 1 7 7 6 7 0 7 6 6 4 1 6 1 6 0 6 7 6 1\n",
            " 6 1 1 1 1 6 3 1 6 1 7 6 1 1 1 7 1 1 1 2 6 6 7 1 6 0 6 0 1 6 1 1 6 0 7 1 6\n",
            " 1 6 6 2 0 6 1 0 6 7 3 6 6 1 6 7 1 1 0 6 1 7 6 7 1 1 1 6 1 1 6 0 1 1 1 3 1\n",
            " 6 1 6 1 1 6 6 6 1 3 0 6 6 6 1 1 6 1 7 7 4 1 6 1 1 1 0 6 6 6 6 6 7 0 6 6 1\n",
            " 6 6 6 6 1 7 6 6 6 7 6 0 3 1 1 1 6 7 7 1 1 1 1 4 6 6 6 0 6 1 6 6 1 1 7 1 6\n",
            " 1 6 1 1 1 1 1 6 1 1 1 1 7 0 1 7 0 1 1]\n",
            "['calm', 'sad', 'sad', 'calm', 'calm', 'sad', 'calm', 'sad', 'sad', 'calm', 'calm', 'calm', 'calm', 'calm', 'calm', 'sad', 'sad', 'disgust', 'calm', 'sad', 'surprised', 'calm', 'neutral', 'sad', 'angry', 'calm', 'calm', 'calm', 'calm', 'angry', 'sad', 'calm', 'calm', 'calm', 'angry', 'sad', 'calm', 'sad', 'calm', 'angry', 'fearful', 'calm', 'calm', 'calm', 'sad', 'calm', 'calm', 'sad', 'sad', 'calm', 'calm', 'sad', 'angry', 'angry', 'sad', 'calm', 'surprised', 'calm', 'calm', 'calm', 'surprised', 'fearful', 'sad', 'sad', 'calm', 'sad', 'sad', 'angry', 'sad', 'angry', 'calm', 'sad', 'sad', 'sad', 'calm', 'sad', 'calm', 'disgust', 'sad', 'sad', 'calm', 'sad', 'sad', 'surprised', 'calm', 'calm', 'sad', 'calm', 'calm', 'sad', 'sad', 'surprised', 'sad', 'sad', 'calm', 'surprised', 'calm', 'calm', 'calm', 'sad', 'sad', 'calm', 'calm', 'sad', 'calm', 'surprised', 'sad', 'calm', 'calm', 'calm', 'sad', 'calm', 'calm', 'calm', 'calm', 'surprised', 'fearful', 'sad', 'surprised', 'angry', 'disgust', 'sad', 'surprised', 'sad', 'surprised', 'surprised', 'calm', 'sad', 'surprised', 'calm', 'surprised', 'surprised', 'sad', 'surprised', 'angry', 'surprised', 'sad', 'sad', 'happy', 'calm', 'sad', 'calm', 'sad', 'angry', 'sad', 'surprised', 'sad', 'calm', 'sad', 'calm', 'calm', 'calm', 'calm', 'sad', 'fearful', 'calm', 'sad', 'calm', 'surprised', 'sad', 'calm', 'calm', 'calm', 'surprised', 'calm', 'calm', 'calm', 'disgust', 'sad', 'sad', 'surprised', 'calm', 'sad', 'angry', 'sad', 'angry', 'calm', 'sad', 'calm', 'calm', 'sad', 'angry', 'surprised', 'calm', 'sad', 'calm', 'sad', 'sad', 'disgust', 'angry', 'sad', 'calm', 'angry', 'sad', 'surprised', 'fearful', 'sad', 'sad', 'calm', 'sad', 'surprised', 'calm', 'calm', 'angry', 'sad', 'calm', 'surprised', 'sad', 'surprised', 'calm', 'calm', 'calm', 'sad', 'calm', 'calm', 'sad', 'angry', 'calm', 'calm', 'calm', 'fearful', 'calm', 'sad', 'calm', 'sad', 'calm', 'calm', 'sad', 'sad', 'sad', 'calm', 'fearful', 'angry', 'sad', 'sad', 'sad', 'calm', 'calm', 'sad', 'calm', 'surprised', 'surprised', 'happy', 'calm', 'sad', 'calm', 'calm', 'calm', 'angry', 'sad', 'sad', 'sad', 'sad', 'sad', 'surprised', 'angry', 'sad', 'sad', 'calm', 'sad', 'sad', 'sad', 'sad', 'calm', 'surprised', 'sad', 'sad', 'sad', 'surprised', 'sad', 'angry', 'fearful', 'calm', 'calm', 'calm', 'sad', 'surprised', 'surprised', 'calm', 'calm', 'calm', 'calm', 'happy', 'sad', 'sad', 'sad', 'angry', 'sad', 'calm', 'sad', 'sad', 'calm', 'calm', 'surprised', 'calm', 'sad', 'calm', 'sad', 'calm', 'calm', 'calm', 'calm', 'calm', 'sad', 'calm', 'calm', 'calm', 'calm', 'surprised', 'angry', 'calm', 'surprised', 'angry', 'calm', 'calm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def make_csv(unlabeled_file_name, y_predicted_label):\n",
        "  with open('prediction.csv', mode='w', newline='') as file:\n",
        "      writer = csv.writer(file)\n",
        "      writer.writerow(['filename', 'label'])\n",
        "      for i in range(len(unlabeled_file_name)):\n",
        "        writer.writerow([unlabeled_file_name[i], y_predicted_label[i]])\n",
        "  file.close()\n",
        "\n",
        "make_csv(y_test, y_predicted_label)"
      ],
      "metadata": {
        "id": "kyxLuEdLzww7"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}