{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMRWN6ilHChk"
   },
   "source": [
    "Containis function for feature extraction. Takes a directory and a boolean indicating wheather to split the data or not. For cross validation use the data directory (labeled data) and split = true. Run all the way to the cell that prints out accuracy score. For testing model on the unlabled data in test folder (those that are named sample001, etc), run the cells below the accuracy score cell will give you a .csv file ready for submission on kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtOOW7Pnxin7",
    "outputId": "40bcd8cc-1e78-4676-fdf6-e9a1def99834"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a-OF3nfHOSmz"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d49e72fde250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "import re\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn import svm\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HSN_P0dCypm1"
   },
   "outputs": [],
   "source": [
    "# test data and test data directory \n",
    "\n",
    "data_dir = \"/content/drive/MyDrive/ELEC378_FinalProject/data/data\"\n",
    "\n",
    "test_dir = \"/content/drive/MyDrive/ELEC378_FinalProject/test/test\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Msvv8n4hfk2h"
   },
   "outputs": [],
   "source": [
    "def FeatureExtraction (dir, split=True): \n",
    " \n",
    "  '''\n",
    "  Function for feature extraction. Change this section to change what features we are using. \n",
    "\n",
    "  input: a directory of the data. Will split to test and train, a boolean: if split is true, will extract features and labels and store them in 4 arrays:\n",
    "  train_data, train_label, test_data, test_label. if split is false, function will extract all data provided in the directory and output 2 arrays with values 2 arrays that are empty. \n",
    "  Use labeled data and split = false will give you  (train_data, train_label); use unlabled data and split = false will give you (test_data, test_label).\n",
    "\n",
    "  output: data matrix and lables for both test and train. \n",
    "\n",
    "  '''\n",
    "  audio_files = [f for f in os.listdir(dir) if f.endswith(\".wav\")]\n",
    "  \n",
    "  train_files, test_files = train_test_split(audio_files, test_size=0.2)\n",
    "\n",
    "  train_data = []\n",
    "  train_label= []\n",
    "  test_data = []\n",
    "  test_label = []  \n",
    "  \n",
    "  \n",
    "  for file_name in audio_files:\n",
    "\n",
    "      # Load audio\n",
    "      file_path = os.path.join(dir, file_name)\n",
    "      raw_audio, sr = librosa.load(file_path)\n",
    "\n",
    "      # if audio is too short, append zeros after it. \n",
    "      if np.shape(raw_audio)[0] < 80000: \n",
    "          padded_audio = np.pad(raw_audio, [(0, 80000 - np.shape(raw_audio)[0])], mode='constant')\n",
    "      else:\n",
    "          padded_audio = raw_audio\n",
    "\n",
    "      # now slice so that we guarantee that each array has the same length\n",
    "      audio = padded_audio[20000:80000]\n",
    "\n",
    "      # feature extraction\n",
    "\n",
    "      mfccs = librosa.feature.mfcc(y=audio, sr = 22050, n_mfcc = 20)\n",
    "      #chroma_stft = librosa.feature.chroma_stft(y = audio, sr=22050)\n",
    "      #mel_spec = librosa.feature.melspectrogram(y = audio, sr = 22050, n_mels = 5)\n",
    "      #gfccs = librosa.feature.gfcc(y, sr=sr, n_mfcc=20)\n",
    "\n",
    "      # Concatenate\n",
    "      features = np.concatenate([mfccs.flatten()])\n",
    "      \n",
    "      label = file_name.split(\".\")[0]\n",
    "\n",
    "      if label[:-3] == \"sample\":\n",
    "        test_data.append(features)\n",
    "        test_label.append(label)\n",
    "\n",
    "      else:\n",
    "        if split: \n",
    "          if file_name in train_files:\n",
    "            train_data.append(features)\n",
    "            train_label.append(label[:-3])\n",
    "\n",
    "          elif file_name in test_files: \n",
    "            test_data.append(features)\n",
    "            test_label.append(label[:-3])\n",
    "\n",
    "        elif not split: \n",
    "          \n",
    "          train_data.append(features)\n",
    "          train_label.append(label[:-3])\n",
    "  \n",
    "\n",
    "  train_data = np.array(train_data)\n",
    "  train_label = np.array(train_label)\n",
    "  test_data = np.array(test_data)\n",
    "  test_label = np.array(test_label) \n",
    "\n",
    "  \n",
    "  return train_data, train_label, test_data, test_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VzAiCJ7FqnpP",
    "outputId": "17a3858a-32bb-43e6-bbf2-952cbf63dd36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = FeatureExtraction(data_dir, split = True)\n",
    "print(np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6LLcKl6kjI1m",
    "outputId": "3afcfc96-a112-437a-9849-ab037cf047fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BLdzBAb9h5_m"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oezh9Evx646j",
    "outputId": "9d7f1dff-b76a-44fd-f7de-792227899f17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584070796460177\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(kernel = \"linear\"))\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted= clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predicted)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7U35fy5TJN0"
   },
   "source": [
    "Below is for unlabeled test data and importing a csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "tCIybaBhTT5O"
   },
   "outputs": [],
   "source": [
    "# extract all labeled data\n",
    "X_train, y_train, dummy1, dummy2 = FeatureExtraction(data_dir, split = False)\n",
    "# extract all unlabeled data\n",
    "dummy1, dummy2, X_test, y_test = FeatureExtraction(test_dir, split = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RZbWKLWGM_R"
   },
   "source": [
    "Training and testing models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "n1M81QJpnM2g"
   },
   "outputs": [],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(kernel = \"linear\"))\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predicted= clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "L7Gjd7waqXsJ"
   },
   "outputs": [],
   "source": [
    "# this cell is to export a .csv file to submit (filename & predicted label) \n",
    "\n",
    "import csv\n",
    "\n",
    "with open('prediction.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['filename', 'label'])\n",
    "    for i in range(len(y_test)):\n",
    "      writer.writerow([y_test[i], y_predicted[i]])\n",
    "file.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
